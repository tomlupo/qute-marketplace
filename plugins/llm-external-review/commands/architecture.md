# /llm-external-review:architecture

> **This command invokes EXTERNAL AI (Gemini) for architecture review. Claude must NOT do this review itself.**

Review overall architecture and design decisions using Gemini's 1M token context.

## Usage

```
/llm-external-review:architecture [<directory>] [--scope <scope>]
```

## Arguments

- `<directory>` - (Optional) Directory to analyze (default: current project)
- `--scope` - (Optional) Focus: full, api, data, ml, frontend

## Behavior

1. **Analyze project structure**
   - Scan directory tree
   - Identify key components
   - Map dependencies

2. **Identify patterns**
   - Design patterns in use
   - Architectural style (monolith, microservices, etc.)
   - Layer separation

3. **Review against best practices**
   - SOLID principles
   - DRY/KISS
   - Separation of concerns

4. **Generate architecture review**:
   ```
   ğŸ—ï¸ Architecture Review
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   ## Project Structure
   ```
   src/
   â”œâ”€â”€ api/          # REST API layer
   â”œâ”€â”€ core/         # Business logic
   â”œâ”€â”€ data/         # Data access
   â”œâ”€â”€ ml/           # ML models
   â””â”€â”€ utils/        # Shared utilities
   ```

   ## Patterns Identified
   - âœ… Repository pattern (data layer)
   - âœ… Service layer (core)
   - âš ï¸ Some controllers have business logic

   ## Strengths
   - Clear separation of API and core
   - Good use of dependency injection
   - Consistent naming conventions

   ## Concerns

   ### ğŸ”´ Critical
   - Circular dependency: core â†’ ml â†’ core

   ### ğŸŸ¡ Improvements
   - api/handler.py: 500+ lines, consider splitting
   - No clear error handling strategy

   ## Recommendations
   1. Extract shared ML utils to break cycle
   2. Split large handlers into sub-modules
   3. Implement centralized error handling

   ## Dependency Graph
   ```
   api â†’ core â†’ data
           â†“
          ml â† (circular!)
   ```
   ```

## Example

```
/llm-external-review:architecture --scope ml

# Output:
ğŸ—ï¸ ML Architecture Review
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ML Components
- models/: 5 model definitions
- training/: Training pipelines
- inference/: Prediction services
- evaluation/: Metrics & validation

## Patterns
- âœ… Model registry pattern
- âœ… Feature store abstraction
- âš ï¸ Training scripts not using config

## Concerns
- No model versioning strategy
- Inference coupled to training code

## Recommendations
1. Add MLflow for experiment tracking
2. Separate inference into standalone service
3. Implement feature validation
```
